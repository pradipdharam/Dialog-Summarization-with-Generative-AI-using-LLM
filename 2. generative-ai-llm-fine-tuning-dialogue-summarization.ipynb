{"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96}],"colab":{"name":"Fine-tune a language model","provenance":[]},"instance_type":"ml.m5.2xlarge","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune a Generative AI Model for Dialogue Summarization","metadata":{"tags":[]}},{"cell_type":"markdown","source":"In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.","metadata":{}},{"cell_type":"markdown","source":"# Summary of my learnings after performing below fine tuning exercise\n* Got the fair idea of how PEFT'ed models gives performance very close to fully trained LLM giving the cost benefit of lesser resources required for training\n* Learnings related to fully fine tuned LLM and PEFT LLM model preparation for inference shared below\n#### Full finetuned LLM model preparation for inference\n* For full fine tuning of the model, create a prompt with prepent the dialog by instruction at the begining as \"Summarize the conversation\" and append the \"Summary:\" to the end of the dialogue. \n* Consider these promps as x_trains, and y_trains as original labels i.e. \"summary\" in the original dataset. <br><br>\n* start_prompt = 'Summarize the following conversation.\\n\\n'\n* end_prompt = '\\n\\nSummary: '\n* prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n* example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n* example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n\n#### PEFT LLM model preparation for inference\n* For PEFT, here LoRA (Low rank adaption) is used. \n* Get the lora adapter using the original model first with  \"peft_model = get_peft_model(original_model, lora_config)\" by passing the lora_configs. \n* Then, train the PEFT adapter using \"peft_trainer = Trainer(model=peft_model,args= peft_training_args, train_dataset=tokenized_datasets[\"train\"])\n* Train the peft_trainer and save the adapter model to './peft-dialogue-summary-checkpoint-from-s3/' . \n* Prepare this adapter model by adding it with original FLAN-T5 model as model_peft_adapter_combined_with_original\n* “model_peft_adapter_combined_with_original = PeftModel.from_pretrained(original_model_base, './peft-dialogue-summary-checkpoint-from-s3/', torch_dtype=torch.bfloat16, is_trainable=False)”\n* Then use the model_peft_adapter_combined_with_original model for evaluation.\n\n#### ROUGE metrics\n* ROUGE performance metric measures or evaluates the performance of predictions as compared to the original labels in case of text\n* when my label itself is text, in that case ROUGE metrics can be used as performance metric to evaluate that supervised learning model.","metadata":{}},{"cell_type":"markdown","source":"##### Citations and Credits\n- This is the hands-on execise lab assignment completed as part of \"Generative AI with Large Language Models\" couse on Coursera.com.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n  - [ 1.2 - Load Dataset and LLM](#1.2)\n  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n- [ 2 - Perform Full Fine-Tuning](#2)\n  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n  - [ 3.2 - Train PEFT Adapter](#3.2)\n  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM","metadata":{}},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"tags":[]},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (23.2.1)\n\n\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\n\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\n\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\npytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n\npytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n\nspyder 4.0.1 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n\nspyder 4.0.1 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n\nnotebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 25.1.0 which is incompatible.\n\npathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\n\npathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\n\nsparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.7 which is incompatible.\n\nspyder 4.0.1 requires jedi==0.14.1, but you have jedi 0.18.2 which is incompatible.\u001b[0m\u001b[31m\n\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"markdown","source":"Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np","metadata":{"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### 1.2 - Load Dataset and LLM\n\nYou are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. ","metadata":{"tags":[]}},{"cell_type":"code","source":"huggingface_dataset_name = \"knkarthick/dialogsum\"\n\ndataset = load_dataset(huggingface_dataset_name)\n\ndataset","metadata":{"tags":[]},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"062c897afba74d5f9d16aeb0fce0f70f","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-391706c81424fc80/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"653996bb520149ffa38b19d9d4ca0267","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b617b642664446c6a4582df1734c02e4","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f01279eb05e047a89eb31653c9d54e07","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b41cb71863f4c76a460c0dc25a8e95a","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3e2753f02fa4da0b880d03d877d85f9","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-391706c81424fc80/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a6a2f1588784402af392217e024f1ed","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 12460\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 1500\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 500\n","    })\n","})"]},"metadata":{}}]},{"cell_type":"markdown","source":"Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model.","metadata":{"tags":[]}},{"cell_type":"code","source":"model_name='google/flan-t5-base'\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"tags":[]},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2452f16a3c6b440e8618dba1358a1577","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b328f88497e84b7ca6e2a3cc0930b024","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cba0171a2c2e475e86fd122ab07a171e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79f4d759dcbd44eea0bd662492ed772e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1274c6bc9cdd4dda8ffe2d36ed168425","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d312d72dbbc4e73a574584993f6c13f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df904c62b3c84715a78527bf8663144f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":"It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. ","metadata":{"tags":[]}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"trainable model parameters: 247577856\n\nall model parameters: 247577856\n\npercentage of trainable model parameters: 100.00%\n"}]},{"cell_type":"markdown","source":"<a name='1.3'></a>\n### 1.3 - Test the Model with Zero Shot Inferencing\n\nTest the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand.","metadata":{"tags":[]}},{"cell_type":"code","source":"index = 200\n\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    original_model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=200,\n    )[0], \n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"---------------------------------------------------------------------------------------------------\n\nINPUT PROMPT:\n\n\n\nSummarize the following conversation.\n\n\n\n#Person1#: Have you considered upgrading your system?\n\n#Person2#: Yes, but I'm not sure what exactly I would need.\n\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n\n#Person2#: That would be a definite bonus.\n\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n\n#Person2#: How can we do that?\n\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n\n#Person2#: No.\n\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n\n#Person2#: That sounds great. Thanks.\n\n\n\nSummary:\n\n\n\n---------------------------------------------------------------------------------------------------\n\nBASELINE HUMAN SUMMARY:\n\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n\n\n---------------------------------------------------------------------------------------------------\n\nMODEL GENERATION - ZERO SHOT:\n\n#Person1#: I'm thinking of upgrading my computer.\n"}]},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Perform Full Fine-Tuning","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='2.1'></a>\n### 2.1 - Preprocess the Dialog-Summary Dataset\n\nYou need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n\nTraining prompt (dialogue):\n```\nSummarize the following conversation.\n\n    Chris: This is his part of the conversation.\n    Antje: This is her part of the conversation.\n    \nSummary: \n```\n\nTraining response (summary):\n```\nBoth Chris and Antje participated in the conversation.\n```\n\nThen preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token).","metadata":{"tags":[]}},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])","metadata":{"tags":[]},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/500 [00:00<?, ? examples/s]"]},"metadata":{}}]},{"cell_type":"code","source":"sample_input_id = tokenized_datasets['train']['input_ids'][0:1]\nsample_label = tokenized_datasets['train']['labels'][0:1]\nprint(\"tokenized_datasets: \", tokenized_datasets)\nprint(\"\\nsample_input_id: \", len(sample_input_id[0]), sample_input_id)\nprint(\"\\nsample_label: \", len(sample_label[0]), sample_label)","metadata":{"tags":[]},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"tokenized_datasets:  DatasetDict({\n\n    train: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 12460\n\n    })\n\n    test: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 1500\n\n    })\n\n    validation: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 500\n\n    })\n\n})\n\n\n\nsample_input_id:  512 [[12198, 1635, 1737, 8, 826, 3634, 5, 1713, 345, 13515, 536, 4663, 10, 2018, 6, 1363, 5, 3931, 5, 27, 31, 51, 7582, 12833, 77, 7, 5, 1615, 33, 25, 270, 469, 58, 1713, 345, 13515, 357, 4663, 10, 27, 435, 34, 133, 36, 3, 9, 207, 800, 12, 129, 3, 9, 691, 18, 413, 5, 1713, 345, 13515, 536, 4663, 10, 2163, 6, 168, 6, 25, 43, 29, 31, 17, 141, 80, 21, 305, 203, 5, 148, 225, 43, 80, 334, 215, 5, 1713, 345, 13515, 357, 4663, 10, 27, 214, 5, 27, 2320, 38, 307, 38, 132, 19, 1327, 1786, 6, 572, 281, 217, 8, 2472, 58, 1713, 345, 13515, 536, 4663, 10, 1548, 6, 8, 200, 194, 12, 1792, 2261, 21154, 19, 12, 253, 91, 81, 135, 778, 5, 264, 653, 12, 369, 44, 709, 728, 3, 9, 215, 21, 39, 293, 207, 5, 1713, 345, 13515, 357, 4663, 10, 8872, 5, 1713, 345, 13515, 536, 4663, 10, 1563, 140, 217, 270, 5, 696, 2053, 11, 11581, 320, 1399, 5, 2321, 3, 9, 1659, 6522, 6, 754, 5, 531, 25, 7269, 6, 1363, 5, 3931, 58, 1713, 345, 13515, 357, 4663, 10, 2163, 5, 1713, 345, 13515, 536, 4663, 10, 14627, 53, 19, 8, 1374, 1137, 13, 5084, 1874, 11, 842, 1994, 6, 25, 214, 5, 148, 310, 225, 10399, 5, 1713, 345, 13515, 357, 4663, 10, 27, 31, 162, 1971, 3986, 13, 648, 6, 68, 27, 131, 54, 31, 17, 1727, 12, 4583, 8, 7386, 5, 1713, 345, 13515, 536, 4663, 10, 1548, 6, 62, 43, 2287, 11, 128, 11208, 24, 429, 199, 5, 27, 31, 195, 428, 25, 72, 251, 274, 25, 1175, 5, 1713, 345, 13515, 357, 4663, 10, 8872, 6, 2049, 2472, 5, 20698, 10, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n\n\n\nsample_label:  512 [[1363, 5, 3931, 31, 7, 652, 3, 9, 691, 18, 413, 6, 11, 7582, 12833, 77, 7, 7786, 7, 376, 12, 43, 80, 334, 215, 5, 12833, 77, 7, 31, 195, 428, 128, 251, 81, 70, 2287, 11, 11208, 12, 199, 1363, 5, 3931, 10399, 10257, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)","metadata":{"tags":[]},"execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"Shapes of the datasets:\n\nTraining: (12460, 2)\n\nValidation: (500, 2)\n\nTest: (1500, 2)\n\nDatasetDict({\n\n    train: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 12460\n\n    })\n\n    test: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 1500\n\n    })\n\n    validation: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 500\n\n    })\n\n})\n"}]},{"cell_type":"markdown","source":"To save some time in the lab, you will subsample the dataset:","metadata":{"tags":[]}},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)","metadata":{"tags":[]},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":"Check the shapes of all three parts of the dataset:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)","metadata":{"tags":[]},"execution_count":26,"outputs":[{"name":"stdout","output_type":"stream","text":"Shapes of the datasets:\n\nTraining: (125, 2)\n\nValidation: (5, 2)\n\nTest: (15, 2)\n\nDatasetDict({\n\n    train: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 125\n\n    })\n\n    test: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 15\n\n    })\n\n    validation: Dataset({\n\n        features: ['input_ids', 'labels'],\n\n        num_rows: 5\n\n    })\n\n})\n"}]},{"cell_type":"markdown","source":"The output dataset is ready for fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n\nNow utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.","metadata":{}},{"cell_type":"code","source":"output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-5,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    logging_steps=1,\n    max_steps=1\n)\n\ntrainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation']\n)","metadata":{"tags":[]},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"tags":[]},"execution_count":29,"outputs":[{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n\n  FutureWarning,\n"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 00:00, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>49.250000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1, training_loss=49.25, metrics={'train_runtime': 73.2951, 'train_samples_per_second': 0.109, 'train_steps_per_second': 0.014, 'total_flos': 5478058819584.0, 'train_loss': 49.25, 'epoch': 0.06})"]},"metadata":{}}]},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab.","metadata":{}},{"cell_type":"code","source":"!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/","metadata":{"tags":[]},"execution_count":30,"outputs":[{"name":"stdout","output_type":"stream","text":"download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/training_args.bin to flan-dialogue-summary-checkpoint/training_args.bin\n\ndownload: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/generation_config.json to flan-dialogue-summary-checkpoint/generation_config.json\n\ndownload: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/config.json to flan-dialogue-summary-checkpoint/config.json\n\ndownload: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/scheduler.pt to flan-dialogue-summary-checkpoint/scheduler.pt\n\ndownload: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/rng_state.pth to flan-dialogue-summary-checkpoint/rng_state.pth\n\ndownload: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/trainer_state.json to flan-dialogue-summary-checkpoint/trainer_state.json\n\ndownload: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/pytorch_model.bin to flan-dialogue-summary-checkpoint/pytorch_model.bin\n\ndownload: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/optimizer.pt to flan-dialogue-summary-checkpoint/optimizer.pt\n"}]},{"cell_type":"markdown","source":"The size of the downloaded instruct model is approximately 1GB.","metadata":{"tags":[]}},{"cell_type":"code","source":"!ls -alh ./flan-dialogue-summary-checkpoint/pytorch_model.bin","metadata":{"tags":[]},"execution_count":31,"outputs":[{"name":"stdout","output_type":"stream","text":"-rw-r--r-- 1 root root 945M May 15 10:25 ./flan-dialogue-summary-checkpoint/pytorch_model.bin\n"}]},{"cell_type":"markdown","source":"Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:","metadata":{"tags":[]}},{"cell_type":"code","source":"instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)","metadata":{"tags":[]},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"<a name='2.3'></a>\n### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nAs with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model.","metadata":{}},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')","metadata":{"tags":[]},"execution_count":34,"outputs":[{"name":"stdout","output_type":"stream","text":"---------------------------------------------------------------------------------------------------\n\nBASELINE HUMAN SUMMARY:\n\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\n\nORIGINAL MODEL:\n\n#Person1#: You're going to need a new computer. #Person1#: You could consider adding a painting program to your computer. #Person1#: You're going to need a computer with a CD-ROM drive. #Person1#: You're going to need a computer with a CD-ROM drive. #Person1#: You're going to need a computer with a CD-ROM drive. #Person1#: You're going to need a computer with a CD-ROM drive. #Person1#: You're going to need a computer with a CD-ROM drive. #Person1#: You're going to need a computer with a CD-ROM drive. #Person1#: You're going to need a computer with a CD-ROM drive. #Person1\n\n---------------------------------------------------------------------------------------------------\n\nINSTRUCT MODEL:\n\n#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n"}]},{"cell_type":"markdown","source":"<a name='2.4'></a>\n### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n\nThe [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')","metadata":{"tags":[]},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9162ae0a148434eba5fb5efc6f166ef","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":"Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results.","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\n\nfor _, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n    original_model_summaries.append(original_model_text_output)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n    instruct_model_summaries.append(instruct_model_text_output)\n    \nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\ndf","metadata":{"tags":[]},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>human_baseline_summaries</th>\n","      <th>original_model_summaries</th>\n","      <th>instruct_model_summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n","      <td>This memo is a memo that I need to make to all...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In order to prevent employees from wasting tim...</td>\n","      <td>The memo is to be distributed to all employees...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n","      <td>Employees are required to sign up for the new ...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#Person2# arrives late because of traffic jam....</td>\n","      <td>The following are the responses of Person 1:</td>\n","      <td>#Person2# got stuck in traffic again. #Person1...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n","      <td>The car is driving me too fast.</td>\n","      <td>#Person2# got stuck in traffic again. #Person1...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>#Person2# complains to #Person1# about the tra...</td>\n","      <td>Getting to work on time is a priority for the ...</td>\n","      <td>#Person2# got stuck in traffic again. #Person1...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n","      <td>Masha and Hero are getting divorced.</td>\n","      <td>Masha and Hero are getting divorced. Kate can'...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","      <td>The divorce is going to be final early in the ...</td>\n","      <td>Masha and Hero are getting divorced. Kate can'...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>#Person1# and Kate talk about the divorce betw...</td>\n","      <td>Masha and Hero are having a separation for 2 m...</td>\n","      <td>Masha and Hero are getting divorced. Kate can'...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Person1# and Brian are at the birthday party ...</td>\n","      <td>#Person1#: Happy Birthday, Brian. #Person2#: I...</td>\n","      <td>Brian's birthday is coming. #Person1# invites ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            human_baseline_summaries  \\\n","0  Ms. Dawson helps #Person1# to write a memo to ...   \n","1  In order to prevent employees from wasting tim...   \n","2  Ms. Dawson takes a dictation for #Person1# abo...   \n","3  #Person2# arrives late because of traffic jam....   \n","4  #Person2# decides to follow #Person1#'s sugges...   \n","5  #Person2# complains to #Person1# about the tra...   \n","6  #Person1# tells Kate that Masha and Hero get d...   \n","7  #Person1# tells Kate that Masha and Hero are g...   \n","8  #Person1# and Kate talk about the divorce betw...   \n","9  #Person1# and Brian are at the birthday party ...   \n","\n","                            original_model_summaries  \\\n","0  This memo is a memo that I need to make to all...   \n","1  The memo is to be distributed to all employees...   \n","2  Employees are required to sign up for the new ...   \n","3       The following are the responses of Person 1:   \n","4                    The car is driving me too fast.   \n","5  Getting to work on time is a priority for the ...   \n","6               Masha and Hero are getting divorced.   \n","7  The divorce is going to be final early in the ...   \n","8  Masha and Hero are having a separation for 2 m...   \n","9  #Person1#: Happy Birthday, Brian. #Person2#: I...   \n","\n","                            instruct_model_summaries  \n","0  #Person1# asks Ms. Dawson to take a dictation ...  \n","1  #Person1# asks Ms. Dawson to take a dictation ...  \n","2  #Person1# asks Ms. Dawson to take a dictation ...  \n","3  #Person2# got stuck in traffic again. #Person1...  \n","4  #Person2# got stuck in traffic again. #Person1...  \n","5  #Person2# got stuck in traffic again. #Person1...  \n","6  Masha and Hero are getting divorced. Kate can'...  \n","7  Masha and Hero are getting divorced. Kate can'...  \n","8  Masha and Hero are getting divorced. Kate can'...  \n","9  Brian's birthday is coming. #Person1# invites ...  "]},"metadata":{}}]},{"cell_type":"markdown","source":"Evaluate the models computing ROUGE metrics. Notice the improvement in the results!","metadata":{"tags":[]}},{"cell_type":"code","source":"original_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"tags":[]},"execution_count":37,"outputs":[{"name":"stdout","output_type":"stream","text":"ORIGINAL MODEL:\n\n{'rouge1': 0.1943950719797494, 'rouge2': 0.05517241379310346, 'rougeL': 0.1655336753119011, 'rougeLsum': 0.16603090701593004}\n\nINSTRUCT MODEL:\n\n{'rouge1': 0.41026607717457186, 'rouge2': 0.17840645241958838, 'rougeL': 0.2977022096267017, 'rougeLsum': 0.2987374187518165}\n"}]},{"cell_type":"markdown","source":"The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:","metadata":{}},{"cell_type":"code","source":"results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n\nhuman_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"tags":[]},"execution_count":38,"outputs":[{"name":"stdout","output_type":"stream","text":"ORIGINAL MODEL:\n\n{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n\nINSTRUCT MODEL:\n\n{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n"}]},{"cell_type":"markdown","source":"The results show substantial improvement in all ROUGE metrics:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(instruct_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[]},"execution_count":39,"outputs":[{"name":"stdout","output_type":"stream","text":"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\n\nrouge1: 18.82%\n\nrouge2: 10.43%\n\nrougeL: 13.70%\n\nrougeLsum: 13.69%\n"}]},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n\nNow, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n\nPEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n\nThat said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n\nYou need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)","metadata":{"tags":[]},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Add LoRA adapter layers/parameters to the original LLM to be trained.","metadata":{"tags":[]}},{"cell_type":"code","source":"peft_model = get_peft_model(original_model, \n                            lora_config)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[]},"execution_count":41,"outputs":[{"name":"stdout","output_type":"stream","text":"trainable model parameters: 3538944\n\nall model parameters: 251116800\n\npercentage of trainable model parameters: 1.41%\n"}]},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Train PEFT Adapter\n\nDefine training arguments and create `Trainer` instance.","metadata":{"tags":[]}},{"cell_type":"code","source":"output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    auto_find_batch_size=True,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=1,\n    logging_steps=1,\n    max_steps=1    \n)\n    \npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n)","metadata":{"tags":[]},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"peft_trainer.train()\n\npeft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"tags":[]},"execution_count":43,"outputs":[{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n\n  FutureWarning,\n"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 00:00, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>51.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":["('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n"," './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n"," './peft-dialogue-summary-checkpoint-local/tokenizer.json')"]},"metadata":{}}]},{"cell_type":"code","source":"!ls -ltrh ./peft-dialogue-summary-checkpoint-local","metadata":{"tags":[]},"execution_count":44,"outputs":[{"name":"stdout","output_type":"stream","text":"total 16M\n\n-rw-r--r-- 1 root root  14M Aug  6 07:03 adapter_model.bin\n\n-rw-r--r-- 1 root root  334 Aug  6 07:03 adapter_config.json\n\n-rw-r--r-- 1 root root 2.5K Aug  6 07:03 tokenizer_config.json\n\n-rw-r--r-- 1 root root 2.2K Aug  6 07:03 special_tokens_map.json\n\n-rw-r--r-- 1 root root 2.4M Aug  6 07:03 tokenizer.json\n"}]},{"cell_type":"markdown","source":"That training was performed on a subset of data. To load a fully trained PEFT model, read a checkpoint of a PEFT model from S3.","metadata":{"tags":[]}},{"cell_type":"code","source":"!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/ ","metadata":{"tags":[]},"execution_count":45,"outputs":[{"name":"stdout","output_type":"stream","text":"download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_config.json to peft-dialogue-summary-checkpoint-from-s3/adapter_config.json\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer_config.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer_config.json\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/special_tokens_map.json to peft-dialogue-summary-checkpoint-from-s3/special_tokens_map.json\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer.json\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_model.bin to peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"}]},{"cell_type":"markdown","source":"Check that the size of this model is much less than the original LLM:","metadata":{"tags":[]}},{"cell_type":"code","source":"!ls -altrh ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin","metadata":{"tags":[]},"execution_count":47,"outputs":[{"name":"stdout","output_type":"stream","text":"-rw-r--r-- 1 root root 14M May 15 11:18 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"}]},{"cell_type":"markdown","source":"Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`.","metadata":{"tags":[]}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\npeft_model = PeftModel.from_pretrained(peft_model_base, \n                                       './peft-dialogue-summary-checkpoint-from-s3/', \n                                       torch_dtype=torch.bfloat16,\n                                       is_trainable=False)","metadata":{"tags":[]},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"The number of trainable parameters will be `0` due to `is_trainable=False` setting:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[]},"execution_count":49,"outputs":[{"name":"stdout","output_type":"stream","text":"trainable model parameters: 0\n\nall model parameters: 251116800\n\npercentage of trainable model parameters: 0.00%\n"}]},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nMake inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model.","metadata":{}},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"tags":[]},"execution_count":50,"outputs":[{"name":"stdout","output_type":"stream","text":"---------------------------------------------------------------------------------------------------\n\nBASELINE HUMAN SUMMARY:\n\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\n\nORIGINAL MODEL:\n\n#Pork1: Have you considered upgrading your system? #Person1: Yes, but I'm not sure what exactly I would need. #Person2: I'd like to make up my own flyers and banners for advertising. #Person2: That would be a great idea. #Person1: I'd also like to upgrade my computer. #Person1: I'd like to upgrade my computer to a CD-ROM drive. #Person2: That sounds great.\n\n---------------------------------------------------------------------------------------------------\n\nINSTRUCT MODEL:\n\n#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n\n---------------------------------------------------------------------------------------------------\n\nPEFT MODEL: #Person1# recommends adding a painting program to #Person2#'s software and upgrading hardware. #Person2# also wants to upgrade the hardware because it's outdated now.\n"}]},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). ","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"tags":[]},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>human_baseline_summaries</th>\n","      <th>original_model_summaries</th>\n","      <th>instruct_model_summaries</th>\n","      <th>peft_model_summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n","      <td>This memo should go out by this afternoon.</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In order to prevent employees from wasting tim...</td>\n","      <td>The memo is intended to be distributed to all ...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n","      <td>This memo is to be distributed by the Departme...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#Person2# arrives late because of traffic jam....</td>\n","      <td>#Porn2: I'm a little late. #Porn1: I'm a littl...</td>\n","      <td>#Person2# got stuck in traffic again. #Person1...</td>\n","      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n","      <td>The traffic is congested and the traffic is ba...</td>\n","      <td>#Person2# got stuck in traffic again. #Person1...</td>\n","      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>#Person2# complains to #Person1# about the tra...</td>\n","      <td>The traffic situation in New York City is a lo...</td>\n","      <td>#Person2# got stuck in traffic again. #Person1...</td>\n","      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n","      <td>Masha and Hero are getting divorced. Masha and...</td>\n","      <td>Masha and Hero are getting divorced. Kate can'...</td>\n","      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","      <td>Masha and Hero are getting divorced.</td>\n","      <td>Masha and Hero are getting divorced. Kate can'...</td>\n","      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>#Person1# and Kate talk about the divorce betw...</td>\n","      <td>Masha and Hero are getting divorced.</td>\n","      <td>Masha and Hero are getting divorced. Kate can'...</td>\n","      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Person1# and Brian are at the birthday party ...</td>\n","      <td>Brian's birthday is coming up.</td>\n","      <td>Brian's birthday is coming. #Person1# invites ...</td>\n","      <td>Brian remembers his birthday and invites #Pers...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            human_baseline_summaries  \\\n","0  Ms. Dawson helps #Person1# to write a memo to ...   \n","1  In order to prevent employees from wasting tim...   \n","2  Ms. Dawson takes a dictation for #Person1# abo...   \n","3  #Person2# arrives late because of traffic jam....   \n","4  #Person2# decides to follow #Person1#'s sugges...   \n","5  #Person2# complains to #Person1# about the tra...   \n","6  #Person1# tells Kate that Masha and Hero get d...   \n","7  #Person1# tells Kate that Masha and Hero are g...   \n","8  #Person1# and Kate talk about the divorce betw...   \n","9  #Person1# and Brian are at the birthday party ...   \n","\n","                            original_model_summaries  \\\n","0         This memo should go out by this afternoon.   \n","1  The memo is intended to be distributed to all ...   \n","2  This memo is to be distributed by the Departme...   \n","3  #Porn2: I'm a little late. #Porn1: I'm a littl...   \n","4  The traffic is congested and the traffic is ba...   \n","5  The traffic situation in New York City is a lo...   \n","6  Masha and Hero are getting divorced. Masha and...   \n","7               Masha and Hero are getting divorced.   \n","8               Masha and Hero are getting divorced.   \n","9                     Brian's birthday is coming up.   \n","\n","                            instruct_model_summaries  \\\n","0  #Person1# asks Ms. Dawson to take a dictation ...   \n","1  #Person1# asks Ms. Dawson to take a dictation ...   \n","2  #Person1# asks Ms. Dawson to take a dictation ...   \n","3  #Person2# got stuck in traffic again. #Person1...   \n","4  #Person2# got stuck in traffic again. #Person1...   \n","5  #Person2# got stuck in traffic again. #Person1...   \n","6  Masha and Hero are getting divorced. Kate can'...   \n","7  Masha and Hero are getting divorced. Kate can'...   \n","8  Masha and Hero are getting divorced. Kate can'...   \n","9  Brian's birthday is coming. #Person1# invites ...   \n","\n","                                peft_model_summaries  \n","0  #Person1# asks Ms. Dawson to take a dictation ...  \n","1  #Person1# asks Ms. Dawson to take a dictation ...  \n","2  #Person1# asks Ms. Dawson to take a dictation ...  \n","3  #Person2# got stuck in traffic and #Person1# s...  \n","4  #Person2# got stuck in traffic and #Person1# s...  \n","5  #Person2# got stuck in traffic and #Person1# s...  \n","6  Kate tells #Person2# Masha and Hero are gettin...  \n","7  Kate tells #Person2# Masha and Hero are gettin...  \n","8  Kate tells #Person2# Masha and Hero are gettin...  \n","9  Brian remembers his birthday and invites #Pers...  "]},"metadata":{}}]},{"cell_type":"raw","source":"Compute ROUGE score for this subset of the data. ","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[]},"execution_count":52,"outputs":[{"name":"stdout","output_type":"stream","text":"ORIGINAL MODEL:\n\n{'rouge1': 0.21066698142632723, 'rouge2': 0.08504347826086957, 'rougeL': 0.1971052143598873, 'rougeLsum': 0.20324750695379273}\n\nINSTRUCT MODEL:\n\n{'rouge1': 0.41026607717457186, 'rouge2': 0.17840645241958838, 'rougeL': 0.2977022096267017, 'rougeLsum': 0.2987374187518165}\n\nPEFT MODEL:\n\n{'rouge1': 0.3725351062275605, 'rouge2': 0.12138811933618107, 'rougeL': 0.27620639623170606, 'rougeLsum': 0.2758134870822362}\n"}]},{"cell_type":"markdown","source":"Notice, that PEFT model results are not too bad, while the training process was much easier!","metadata":{}},{"cell_type":"markdown","source":"You already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models.","metadata":{}},{"cell_type":"code","source":"human_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\npeft_model_summaries     = results['peft_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[]},"execution_count":53,"outputs":[{"name":"stdout","output_type":"stream","text":"ORIGINAL MODEL:\n\n{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n\nINSTRUCT MODEL:\n\n{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n\nPEFT MODEL:\n\n{'rouge1': 0.40810631575616746, 'rouge2': 0.1633255794568712, 'rougeL': 0.32507074586565354, 'rougeLsum': 0.3248950182867091}\n"}]},{"cell_type":"markdown","source":"The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n\nCalculate the improvement of PEFT over the original model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[]},"execution_count":54,"outputs":[{"name":"stdout","output_type":"stream","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n\nrouge1: 17.47%\n\nrouge2: 8.73%\n\nrougeL: 12.36%\n\nrougeLsum: 12.34%\n"}]},{"cell_type":"markdown","source":"Now calculate the improvement of PEFT over a full fine-tuned model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[]},"execution_count":55,"outputs":[{"name":"stdout","output_type":"stream","text":"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n\nrouge1: -1.35%\n\nrouge2: -1.70%\n\nrougeL: -1.34%\n\nrougeLsum: -1.35%\n"}]},{"cell_type":"markdown","source":"Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU).","metadata":{}},{"cell_type":"markdown","source":"# Summary of my learnings from the exercise\n* Got the fair idea of how PEFT'ed models gives performance very close to fully trained LLM giving the cost benefit of lesser resources required for training\n* Learnings related to fully fine tuned LLM and PEFT LLM model preparation for inference shared below\n#### Full finetuned LLM model preparation for inference\n* For full fine tuning of the model, create a prompt with prepent the dialog by instruction at the begining as \"Summarize the conversation\" and append the \"Summary:\" to the end of the dialogue. \n* Consider these promps as x_trains, and y_trains as original labels i.e. \"summary\" in the original dataset. <br><br>\n* start_prompt = 'Summarize the following conversation.\\n\\n'\n* end_prompt = '\\n\\nSummary: '\n* prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n* example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n* example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n\n#### PEFT LLM model preparation for inference\n* For PEFT, here LoRA (Low rank adaption) is used. \n* Get the lora adapter using the original model first with  \"peft_model = get_peft_model(original_model, lora_config)\" by passing the lora_configs. \n* Then, train the PEFT adapter using \"peft_trainer = Trainer(model=peft_model,args= peft_training_args, train_dataset=tokenized_datasets[\"train\"])\n* Train the peft_trainer and save the adapter model to './peft-dialogue-summary-checkpoint-from-s3/' . \n* Prepare this adapter model by adding it with original FLAN-T5 model as model_peft_adapter_combined_with_original\n* “model_peft_adapter_combined_with_original = PeftModel.from_pretrained(original_model_base, './peft-dialogue-summary-checkpoint-from-s3/', torch_dtype=torch.bfloat16, is_trainable=False)”\n* Then use the model_peft_adapter_combined_with_original model for evaluation.\n\n#### ROUGE metrics\n* ROUGE performance metric measures or evaluates the performance of predictions as compared to the original labels in case of text\n* when my label itself is text, in that case ROUGE metrics can be used as performance metric to evaluate that supervised learning model.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}